---
title: "InfoBatch: Dataset Pruning on the Fly"
date: "2024-01-17"
summary: "Multi‑epoch training wastes time on easy, well‑learned samples. InfoBatch dynamically prunes data and rescales the loss to keep accuracy while speeding up training by 20–40% across vision and language tasks."
---

[Zangwei Zheng](https://zhengzangw.github.io/about/), zangwei@u.nus.edu  
National University of Singapore

**ICLR 2024 Oral**  
Other version: [[arXiv](https://arxiv.org/abs/2307.02047)] [[Code](https://github.com/NUS-HPC-AI-Lab/InfoBatch)] [[中文](https://zhuanlan.zhihu.com/p/678510967)]  
Discuss on [X](https://twitter.com/zangweizheng/status/1747857745108508746?s=61&t=n26cAQB4vNMc4lYtIfXdMQ) with the author.

## TL;DR

Training for many epochs wastes time on easy, well‑learned samples. InfoBatch speeds things up by dynamically pruning data and rescaling the loss to keep performance. It delivers 20–40% faster training on image classification, semantic segmentation, vision pretraining, diffusion models, and LLM instruction fine‑tuning—without losing accuracy.

![overview](/blogs/images/infobatch-overview.png)

## How does InfoBatch work?

We provide a plug‑and‑play PyTorch implementation for InfoBatch (under active development). With the three changes shown below, you can plug InfoBatch into your training code.

![code](/blogs/images/infobatch-code.jpg)

Here is a brief overview of the InfoBatch algorithm.

- First, InfoBatch randomly drops a fraction $(1 - ratio)$ of samples whose loss is below the average loss over the batch. The paper discusses more advanced strategies, but this simple rule already works very well.
- Second, for the remaining below‑average‑loss samples, InfoBatch rescales their loss by $(1 - ratio)^{-1}$ to keep overall training unbiased.
- Third, at the end of training, InfoBatch runs through all samples once to mitigate forgetting.

The hyperparameter $delta$ controls the fraction of epochs that perform on‑the‑fly pruning. A good starting point is $ratio = 0.5, delta = 0.875$.

In the code above: (1) the dataset is wrapped and the index order is managed, (2) the InfoBatch sampler is passed to the DataLoader constructor, and (3) the loss is rescaled and the sampler is updated with the loss between the forward and backward pass. For more mathematical discussion and ablations, see the [paper](https://arxiv.org/abs/2307.02047). For parallel training, see the [code](https://github.com/NUS-HPC-AI-Lab/InfoBatch).

## Applications

The idea behind InfoBatch is simple but effective across many applications.

- Image classification: 40% speedup with no accuracy drop, unlike prior methods.
- MAE pretraining: 20% time saved for ViT and Swin, with no downstream accuracy loss.
- Semantic segmentation: 40% time saved with no mIoU degradation.
- Diffusion models: 27% time saved with comparable FID.
- LLM instruction fine‑tuning: 20% time saved.
